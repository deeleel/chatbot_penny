{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import torch\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "seed = 10\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/prepared_with_context+label+negative.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_response</th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh, hi!</td>\n",
       "      <td>oh hi</td>\n",
       "      <td>two hundred pound transvestite with a skin co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi?</td>\n",
       "      <td>hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oh, that’s nice.</td>\n",
       "      <td>oh that’s nice</td>\n",
       "      <td>we don’t mean to interrupt we live across the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oh, okay, well, guess I’m your new neighbour,...</td>\n",
       "      <td>oh okay well guess i’m your new neighbour penny</td>\n",
       "      <td>oh… uh… no… we don’t live together… um… we li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>hi</td>\n",
       "      <td>leonard sheldon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29899</th>\n",
       "      <td>Well, then you get it.</td>\n",
       "      <td>well then you get it</td>\n",
       "      <td>well sometimes women don’t care sometimes it ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29900</th>\n",
       "      <td>Right.</td>\n",
       "      <td>right</td>\n",
       "      <td>okay um let’s try this think of yourself as o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29901</th>\n",
       "      <td>Let’s forget the toy thing, okay? Um, maybe…</td>\n",
       "      <td>let’s forget the toy thing okay um maybe…</td>\n",
       "      <td>well then you get it [SEP] because there’s on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29902</th>\n",
       "      <td>All right. What do you think is happening?</td>\n",
       "      <td>all right what do you think is happening</td>\n",
       "      <td>right [SEP] although amy’s already taken me o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29903</th>\n",
       "      <td>Don’t look at me like that, I tried.</td>\n",
       "      <td>don’t look at me like that i tried</td>\n",
       "      <td>let’s forget the toy thing okay um maybe… [SE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29904 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_response   \n",
       "0                                                Oh, hi!  \\\n",
       "1                                                    Hi?   \n",
       "2                                       Oh, that’s nice.   \n",
       "3       Oh, okay, well, guess I’m your new neighbour,...   \n",
       "4                                                    Hi.   \n",
       "...                                                  ...   \n",
       "29899                             Well, then you get it.   \n",
       "29900                                             Right.   \n",
       "29901       Let’s forget the toy thing, okay? Um, maybe…   \n",
       "29902         All right. What do you think is happening?   \n",
       "29903               Don’t look at me like that, I tried.   \n",
       "\n",
       "                                               response   \n",
       "0                                                 oh hi  \\\n",
       "1                                                    hi   \n",
       "2                                        oh that’s nice   \n",
       "3       oh okay well guess i’m your new neighbour penny   \n",
       "4                                                    hi   \n",
       "...                                                 ...   \n",
       "29899                              well then you get it   \n",
       "29900                                             right   \n",
       "29901         let’s forget the toy thing okay um maybe…   \n",
       "29902          all right what do you think is happening   \n",
       "29903                don’t look at me like that i tried   \n",
       "\n",
       "                                                 context  label  \n",
       "0       two hundred pound transvestite with a skin co...      1  \n",
       "1                                                     hi      1  \n",
       "2       we don’t mean to interrupt we live across the...      1  \n",
       "3       oh… uh… no… we don’t live together… um… we li...      1  \n",
       "4                                        leonard sheldon      1  \n",
       "...                                                  ...    ...  \n",
       "29899   well sometimes women don’t care sometimes it ...      1  \n",
       "29900   okay um let’s try this think of yourself as o...      1  \n",
       "29901   well then you get it [SEP] because there’s on...      1  \n",
       "29902   right [SEP] although amy’s already taken me o...      1  \n",
       "29903   let’s forget the toy thing okay um maybe… [SE...      1  \n",
       "\n",
       "[29904 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Di\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import  nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_symbols(x):\n",
    "    pattern = r'[\\\"|\\#|\\$|\\%|\\&|\\(|\\)|\\*|\\+|\\,|\\-|\\/|\\:|\\;|\\<|\\=|\\>|\\@|\\\\|\\^|\\_|\\`|\\{|\\||\\}|\\~|\\.|\\!|\\?]'\n",
    "    cleaned = re.sub(pattern=pattern, repl='', string=x)\n",
    "    return cleaned.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSearchEngine:\n",
    "    def __init__(self, text_database: list[str], isFaiss):\n",
    "        self.raw_procesed_data = [self.preprocess(sample) for sample in text_database]\n",
    "        self.base = []\n",
    "        self.retriever = None\n",
    "        self.inverted_index = {}\n",
    "        self.isFaiss = isFaiss\n",
    "        self._init_retriever(text_database)\n",
    "        self._init_inverted_index(text_database)\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(sentence: str) -> str:\n",
    "        sentence = clean_symbols(sentence)\n",
    "        return sentence\n",
    "    \n",
    "    def _init_faiss(self):\n",
    "        self.index = faiss.IndexFlatL2(self.base.shape[1])\n",
    "        faiss.normalize_L2(self.base.toarray().astype('float32'))\n",
    "        self.index.add(self.base.toarray().astype('float32'))\n",
    "\n",
    "    def _init_retriever(self, text_database: list[str]):\n",
    "        self.retriever = TfidfVectorizer(stop_words=russian_stopwords,\n",
    "                             ngram_range=(1,3),\n",
    "                             max_features=4096, # берем в словарь слова с максимальными tf\n",
    "                             tokenizer=wordpunct_tokenize)\n",
    "        self.base = self.retriever.fit_transform(text_database) # подсчитать вектора = представление документа\n",
    "        if self.isFaiss:\n",
    "            self._init_faiss()\n",
    "\n",
    "\n",
    "    def retrieve(self, query: str) -> np.array:\n",
    "        return self.retriever.transform([query]) # векторизовать входное значение\n",
    "\n",
    "    def retrieve_documents(self, query: str, top_k=3) -> np.array:\n",
    "        query_vector = self.retrieve(query)\n",
    "        cosine_similarities = cosine_similarity(query_vector, self.base).flatten()\n",
    "        relevant_indices = np.argsort(cosine_similarities, axis=0)[::-1][:top_k]\n",
    "        return relevant_indices\n",
    "    \n",
    "    def retrieve_documents_faiss(self, query: str, top_k=3):\n",
    "        query_vector = self.retrieve(query).toarray().astype('float32')\n",
    "        faiss.normalize_L2(query_vector)\n",
    "        _, relevant_indices = self.index.search(query_vector, k=top_k)\n",
    "        return relevant_indices[0]\n",
    "\n",
    "    def _init_inverted_index(self, text_database: list[str]):\n",
    "        self.inverted_index = dict(enumerate(text_database))\n",
    "\n",
    "    def display_relevant_docs(self, query: str, full_database, top_k=3) -> list[str]:\n",
    "        query = clean_symbols(query)\n",
    "\n",
    "        if self.isFaiss:\n",
    "            docs_indexes = self.retrieve_documents_faiss(query, top_k)\n",
    "        else:\n",
    "            docs_indexes = self.retrieve_documents(query, top_k)\n",
    "            \n",
    "        return [self.inverted_index[ind] for ind in docs_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Di\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "simple_search_engine = SimpleSearchEngine(df['context'], isFaiss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chatbot_response(query):\n",
    "    simple_search_engine_results = simple_search_engine.display_relevant_docs(query, df['context'])\n",
    "    response = df[df['context'] == simple_search_engine_results[0]]['original_response'].values\n",
    "    return response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Hi\n",
      "--- Hi?\n",
      "\n",
      "\n",
      "-Nice to meet you\n",
      "--- Good.\n",
      "\n",
      "\n",
      "-What are you doing?\n",
      "--- I’m settling once and for all who is the smartest around here. Okay, are you ready?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_context = ''\n",
    "count = 0\n",
    "for i in range(3):\n",
    "    query = input()\n",
    "    if (count == 0) or (count % 2 == 0):\n",
    "        all_context = query\n",
    "    else:\n",
    "        all_context = all_context + ' [SEP] ' + query\n",
    "    count += 1\n",
    "    \n",
    "    print('-' + query)\n",
    "    print('---' + get_chatbot_response(all_context))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
